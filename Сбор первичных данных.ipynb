{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_part_1 = \"https://samara.cian.ru/cat.php?deal_type=sale&engine_version=2&offer_type=flat&p=\"\n",
    "page_part_2 = \"&region=4966&room1=1&with_neighbors=0\"\n",
    "\n",
    "all_hrefs = [ ]  \n",
    "\n",
    "for i in range(1,54): \n",
    "    page = page_part_1 + str(i) + page_part_2\n",
    "    response = requests.get(page.format(1))\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    hrefs = soup.findAll('div', attrs = {'class':\"_93444fe79c--content--2IC7j\"})\n",
    "    clean_hrefs = [item.a.attrs['href'] for item in hrefs]\n",
    "    all_hrefs.extend(clean_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_part_1 = \"https://samara.cian.ru/cat.php?deal_type=sale&engine_version=2&offer_type=flat&p=\"\n",
    "page_part_2 = \"&region=4966&room2=1&with_neighbors=0\"\n",
    "\n",
    "\n",
    "for i in range(1,54): \n",
    "    page = page_part_1 + str(i) + page_part_2\n",
    "    response = requests.get(page.format(1))\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    # Забираем себе хрефы и очищаем их\n",
    "    hrefs = soup.findAll('div', attrs = {'class':\"_93444fe79c--content--2IC7j\"})\n",
    "    clean_hrefs = [item.a.attrs['href'] for item in hrefs]\n",
    "    all_hrefs.extend(clean_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_part_1 = \"https://samara.cian.ru/cat.php?deal_type=sale&engine_version=2&offer_type=flat&p=\"\n",
    "page_part_2 = \"&region=4966&room3=1&with_neighbors=0\"\n",
    "\n",
    "\n",
    "for i in range(1,54): \n",
    "    page = page_part_1 + str(i) + page_part_2\n",
    "    response = requests.get(page.format(1))\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    # Забираем себе хрефы и очищаем их\n",
    "    hrefs = soup.findAll('div', attrs = {'class':\"_93444fe79c--content--2IC7j\"})\n",
    "    clean_hrefs = [item.a.attrs['href'] for item in hrefs]\n",
    "    all_hrefs.extend(clean_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_part_1 = \"https://samara.cian.ru/cat.php?deal_type=sale&engine_version=2&offer_type=flat&p=\"\n",
    "page_part_2 = \"&region=4966&room4=1&with_neighbors=0\"\n",
    "\n",
    "\n",
    "for i in range(1,54): \n",
    "    page = page_part_1 + str(i) + page_part_2\n",
    "    response = requests.get(page.format(1))\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    # Забираем себе хрефы и очищаем их\n",
    "    hrefs = soup.findAll('div', attrs = {'class':\"_93444fe79c--content--2IC7j\"})\n",
    "    clean_hrefs = [item.a.attrs['href'] for item in hrefs]\n",
    "    all_hrefs.extend(clean_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_part_1 = \"https://samara.cian.ru/cat.php?deal_type=sale&engine_version=2&offer_type=flat&p=\"\n",
    "page_part_2 = \"&region=4966&room7=1&room9=1&with_neighbors=0\"\n",
    "\n",
    "\n",
    "for i in range(1,6): \n",
    "    page = page_part_1 + str(i) + page_part_2\n",
    "    response = requests.get(page.format(1))\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    # Забираем себе хрефы и очищаем их\n",
    "    hrefs = soup.findAll('div', attrs = {'class':\"_93444fe79c--content--2IC7j\"})\n",
    "    clean_hrefs = [item.a.attrs['href'] for item in hrefs]\n",
    "    all_hrefs.extend(clean_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs=pd.DataFrame(all_hrefs, columns=['hrefs'])\n",
    "\n",
    "pd.DataFrame.to_csv(hrefs,'CIAN_data_hrefs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_Flat_Downloader(href):\n",
    "    \"\"\"\n",
    "    Функция производит выкачку по одной ЦИАНовской ссылке \n",
    "    всей существующей информации о квартире.\n",
    "    Ввод: ссылка на описание квартиры\n",
    "    Вывод: словарь с информацией о квартире    \n",
    "    \"\"\"\n",
    "    \n",
    "    data = { }  # Задали пустой словарь, в который мы будем сохранять данные\n",
    "    \n",
    "    # Подгружаем страничку с информацией по квартире\n",
    "    response = requests.get(href)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # Вытаскиваем цену на квартиру\n",
    "    l = soup.findAll('span', itemprop=\"price\")\n",
    "    price = l[0].attrs[\"content\"].replace(' ','')\n",
    "    data['Цена'] = price\n",
    "    \n",
    "\n",
    "    \n",
    "    # Вытаскиваем расстояние до метро то ли пешком то ли на машине\n",
    "    try:\n",
    "        Do_metro = soup.findAll('span', attrs = {'class':\"a10a3f92e9--underground_time--1fKft\"})[0].text.strip()\n",
    "        data['До Метро'] = Do_metro\n",
    "    except Exception:\n",
    "        data['До Метро'] = \"NA\"\n",
    "    \n",
    "    \n",
    "    # Вытаскиваем количество комнат в квартире\n",
    "    komnata=soup.findAll('h1', attrs = {'class':\"a10a3f92e9--title--2Widg\"})[0].text\n",
    "    data['Комнаты'] = komnata\n",
    "    \n",
    "    # Вытаскиваем район квартиры\n",
    "    raion = soup.findAll('a', attrs = {'class':\"a10a3f92e9--link--1t8n1 a10a3f92e9--address-item--1clHr\"})[3].text\n",
    "    data['Район'] = raion\n",
    "    # Вытаскиваем тип жилья\n",
    "    type_ap=soup.findAll('span', attrs = {'class':\"a10a3f92e9--value--3Ftu5\"})[0].text\n",
    "    data['Тип жилья'] = type_ap\n",
    "    # Вытаскиваем жилую площадь\n",
    "    zhil = soup.findAll('div', attrs = {'class':\"a10a3f92e9--info-value--18c8R\"})[1].text\n",
    "    data['Жилая'] =  zhil\n",
    "    # Вытаскиваем плошадь кухни\n",
    "    kitch = soup.findAll('div', attrs = {'class':\"a10a3f92e9--info-value--18c8R\"})[2].text\n",
    "    data['Кухня'] = kitch\n",
    "    # Вытаскиваем этажи\n",
    "    et = soup.findAll('div', attrs = {'class':\"a10a3f92e9--info-value--18c8R\"})[3].text\n",
    "    data['Этаж'] = et\n",
    "    # Вытаскиваем срок сдачи дома \n",
    "    year = soup.findAll('div', attrs = {'class':\"a10a3f92e9--info-value--18c8R\"})[4].text\n",
    "    data['Срок сдачи'] = year\n",
    "    #Вытаскиваем высоту потолков\n",
    "    potolok = soup.findAll('span', attrs = {'class':\"a10a3f92e9--value--3Ftu5\"})[2].text.strip()\n",
    "    data['Высота потолков']=potolok\n",
    "    \n",
    "    sanuzel = soup.findAll('span', attrs = {'class':\"a10a3f92e9--value--3Ftu5\"})[3].text.strip()\n",
    "    data['Санузлы']=sanuzel\n",
    "    \n",
    "    balkon = soup.findAll('span', attrs = {'class':\"a10a3f92e9--value--3Ftu5\"})[4].text.strip()\n",
    "    data['Балконы']=balkon\n",
    "    \n",
    "    view = soup.findAll('span', attrs = {'class':\"a10a3f92e9--value--3Ftu5\"})[5].text.strip()\n",
    "    data['Вид из оконо']=view\n",
    "    \n",
    "    return(data) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# В эту табличку будем собирать данные\n",
    "df = pd.DataFrame( )\n",
    "k = 0 # Это номера наблюдений\n",
    "n = len(all_hrefs)\n",
    "for item in all_hrefs:\n",
    "    k = k + 1\n",
    "    # грузим новое наблюдение\n",
    "    df1 = pd.DataFrame.from_dict(One_Flat_Downloader(item),orient='index')\n",
    "    # присваиваем этому наблюдению номер\n",
    "    df1.columns =[k]\n",
    "    # закидываем его в итоговую таблицу\n",
    "    df = df.join(df1, how='outer')\n",
    "    # Выдавать информацию о том, что сделана каждая десятая итерация! \n",
    "    if k%10 == 0:\n",
    "        print('скачал ', k, ' квартир', n)\n",
    "\n",
    "        \n",
    "df = df.T  # Для удобства транспонируем таблицу        \n",
    "\n",
    "\n",
    "df['Хрефы'] = all_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
